{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dissecting RAVE with `torchbend`\n",
    "\n",
    "Welcome to this tutorial! The idea here is to use the `torchbend` library to dissect the inner guts of a general machine learning model, and especially `RAVE` models that we will learn to bend this afternoon. If you never used a Jupyter Python notebook, its utilisation is quite simple: \n",
    "- Execute the cells containing the python code on by one by clicking on the `Run` button on the top toolbar (or Shift + Enter)\n",
    "- If you feel like it, you can change some of the variables to play a little bit with the code! \n",
    "\n",
    "Try that with the code cell below, and read it carefully : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ok, we are in a Python cell! \n",
    "#Â <- this sharp symbol means that this line is commented, meaning that it is just text and no code.\n",
    "# below, we define some variables using the \"=\" set operator\n",
    "\n",
    "# print is used to output text below the code cell, where the outputs are.\n",
    "print('Testing python stuff : ')\n",
    "\n",
    "\n",
    "# Execute the cell and observe the outputs!\n",
    "# Try changing the variables below, and observe how the output change.\n",
    "number = 3\n",
    "other_number = 4.3\n",
    "string = \"hello world!\"\n",
    "\n",
    "\n",
    "list_of_things = [3, 4, 5, 2]\n",
    "print(list_of_things[1]) # lists are indexed by integrals\n",
    "\n",
    "dictionary = {'a': 3, 'b': 4}\n",
    "print(dictionary['a']) # dictionary indexed by keys\n",
    "\n",
    "\n",
    "# example of conditional codes : \n",
    "if (number == 3):\n",
    "    print('number is three here!')\n",
    "elif (number == 4):\n",
    "    print('number is four here!')\n",
    "else:\n",
    "    print('number is... something')\n",
    "\n",
    "\n",
    "# example of loops : \n",
    "for i in range(4):\n",
    "    print(\"current i value : \", i)\n",
    "# example of looping in elements of a list :\n",
    "for v in list_of_things:\n",
    "    print(v)\n",
    "\n",
    "# defining a function with def\n",
    "def square(x):\n",
    "    return x * x\n",
    "print(3, square(3))\n",
    "print(4, square(4))\n",
    "\n",
    "# defining an object, that is a set of attributes and functions\n",
    "class Object():\n",
    "    def __init__(self, a, b):\n",
    "        # an initialization function\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "\n",
    "    def describe(self):\n",
    "        print(\"a: \", self.a)\n",
    "        print(\"b : \", self.b)\n",
    "\n",
    "obj1 = Object(1, 2)\n",
    "obj2 = Object(\"hello\", \"goodbye\")\n",
    "# call methods\n",
    "obj1.describe()\n",
    "obj2.describe()\n",
    "# get attributes\n",
    "a = obj1.a\n",
    "b = obj2.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a machine learning model in Python?\n",
    "\n",
    "There are many machine learning libraries for Python : [Pytorch](https://pytorch.org/), [Tensorflow](https://www.tensorflow.org/?hl=fr), [jax](https://github.com/jax-ml/jax)... All of these have their own logic, but are always based on a similar architecture of what a ML model is : \n",
    "\n",
    "- a set of **weights**, that are typically trained during a training process\n",
    "- a **computing graph**, that describes how the paramters are used to process the inputs.\n",
    "\n",
    "`torchbend` is a library allowing to analyse both parameters and computing graphs of a machine learning model, and also allows to hack both to perform creative operations to bend existing machine learning models. Let's describe this logic with three steps: \n",
    "1) a dumb machine learning model\n",
    "2) an additive synthesizer\n",
    "3) a pre-trained RAVE model\n",
    "\n",
    "Ready? Let's go!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simple and useless machine learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "# a machine learning model in torch is generally implemented using a torch.nn.Module as below\n",
    "# this class is a dumb module applying two linear transformations : \n",
    "# out = A * x + B\n",
    "# with a non linearity (a simple tanh function) inside. \n",
    "\n",
    "class Foo(torch.nn.Module):\n",
    "    # initialization method\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim):\n",
    "        super().__init__()\n",
    "        # we initialize here two different linear modules, that are two linear transformations of the input (out = A * x + b):\n",
    "        self.linear_1 = torch.nn.Linear(in_dim, hidden_dim)\n",
    "        self.linear_2 = torch.nn.Linear(hidden_dim, out_dim)\n",
    "        # we also init a non-linearity module, called nnlin\n",
    "        self.nnlin = torch.nn.Tanh()\n",
    "\n",
    "    # definition of how the data is processed\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        out = self.linear_1(x)\n",
    "        print(\"first layer shape : \", out.shape)\n",
    "        out = self.linear_2(out)\n",
    "        print(\"second layer shape : \", out.shape)\n",
    "        return out\n",
    "\n",
    "\n",
    "in_dim = 4\n",
    "hidden_dim = 80\n",
    "out_dim = 16\n",
    "\n",
    "# let's create a model\n",
    "module = Foo(in_dim, hidden_dim, out_dim)\n",
    "\n",
    "# let's print this model\n",
    "print(module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this module has three different submodule : \n",
    "- *linear_1*, the first linear transformation (expanding input dimension 4 to 80), \n",
    "- *linear_2*, the second linear transformation (expanding output dimension 80 to 16), \n",
    "- *nnlin*, a simple object representing the `Tanh` function. \n",
    "\n",
    "Let's use this module to process an input : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch numbers is the number of different examples to process\n",
    "n_batch = 4\n",
    "\n",
    "# we create different examples of inputs f `in_dim` dimensions: \n",
    "x = torch.randn(n_batch, in_dim)\n",
    "print('-- input : ')\n",
    "print(x)\n",
    "\n",
    "# process input\n",
    "print(\"-- processing : \")\n",
    "out = module(x)\n",
    "\n",
    "# the output is a set of different examples of `out_dim` dimensions\n",
    "print('-- output : ')\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok ! Now let's see here what are the *parameters*, and what is the *graph* of this simple module with `torchbend`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchbend as tb\n",
    "\n",
    "# wraps the existing module inside a BendedModule object allows to analyse any module\n",
    "bended_module = tb.BendedModule(module)\n",
    "# \"trace\" is needed to analyse the computing graph of our module. \n",
    "bended_module.trace(x=x)\n",
    "\n",
    "# print paramters\n",
    "print('Weights : ')\n",
    "bended_module.print_weights()\n",
    "\n",
    "print('\\nGraphs : ')\n",
    "print(bended_module.graph().print_tabular())\n",
    "\n",
    "print('\\nActivations : ')\n",
    "# print activations\n",
    "bended_module.print_activations();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see, in the output above : \n",
    "- the **weights** of the module, that are for our module the parameters of the linear transformation (A * x + B, where A is the weight and B the bias)\n",
    "- the **graph** of the module, that are all the operations made from the input `x` to the output `output`\n",
    "- the **activations**, that are the *intermediary* values obtained from the input `x`.\n",
    "\n",
    "We could describe the difference between weights and activations like this : \n",
    "\n",
    "![Decomposition of a module](assets/module_decomposition.png)\n",
    "\n",
    "Typically (we mean by that, before we start to mess everything around this afternoon), weights are modified during the model's learning process, but do not change when using the model (in technical terms, during *inference*). The graph describes what operations are done to the model's inputs, and *activations* are all the intermediary values processed by the model's computing graph, and are then different for different inputs. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A additive synthesizer in Pytorch \n",
    "\n",
    "To make it more clear, let's take an example that should speak to you in a slightly less abstract way : an additive synthesizer. Indeed, an additive synthesizer can be described in a similar way than machine learning module, and will allow to make the distinction between *weights* and *activations* clearer. \n",
    "\n",
    "Let's define our additive synthesizer :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "import torch, torch.nn as nn\n",
    "import sys; sys.path.append('..')\n",
    "import torchbend as tb\n",
    "tb.set_output('notebook')\n",
    "\n",
    "class Joseph(nn.Module):\n",
    "    def __init__(self, f0, n_partials, fs=44100):\n",
    "        super().__init__()\n",
    "        self.f0 = nn.Parameter(torch.full((1, 1, 1), f0), requires_grad=False)\n",
    "        self.f_mult = nn.Parameter(torch.arange(1, n_partials+1).unsqueeze(-1), requires_grad=False)\n",
    "        self.amps = nn.Parameter(torch.ones(1, n_partials, 1), requires_grad=False)\n",
    "\n",
    "    def forward(self, t):\n",
    "        t = t.unsqueeze(-2)\n",
    "        freqs = self.f0 * self.f_mult\n",
    "        waves =  torch.sin(2 * torch.pi * freqs * t) \n",
    "        waves = waves * torch.nn.functional.softmax(self.amps, dim=-2)\n",
    "        out = waves.sum(-2)\n",
    "        return out\n",
    "\n",
    "T = 2.0\n",
    "fs = 44100\n",
    "module = Joseph(110, 4, fs)\n",
    "t = torch.linspace(0., T, int(T*fs))\n",
    "\n",
    "wave = module(t[None])\n",
    "Audio(wave.numpy(), rate=fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the module `Joseph` (named after Joseph Fourier of course) takes a `t` input, representing time index for each sample (in seconds), and generates a waveform with the first `n_partials` harmonics. What are the weights here, and what are the activations? Take a time to think, and execute the cell below to get the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchbend as tb\n",
    "\n",
    "# wraps the existing module inside a BendedModule object allows to analyse any module\n",
    "bended_module = tb.BendedModule(module)\n",
    "# \"trace\" is needed to analyse the computing graph of our module. \n",
    "bended_module.trace(t=t)\n",
    "\n",
    "# print paramters\n",
    "print('Weights : ')\n",
    "bended_module.print_weights()\n",
    "\n",
    "print('\\nGraphs : ')\n",
    "print(bended_module.graph().print_tabular())\n",
    "\n",
    "print('\\nActivations : ')\n",
    "# print activations\n",
    "bended_module.print_activations();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, this is a little more complicated, but with a little attention it is quite easy to understand everything. \n",
    "\n",
    "**Weights.** We can see that `Joseph` has three different paramters : \n",
    "- `f0`: fundamental frequency of the module\n",
    "- `f_mult` : the frequency multiplier of each save's partial\n",
    "- `amps`: the weights of each partials.\n",
    "These values describe the **weights** of the model : indeed, they define the module's behavior and do not change across different examples. Typically, these could be trained to learn how to reproduce a sound, in a similar way than [DDSP](https://github.com/magenta/ddsp). \n",
    "\n",
    "**Graph.** A little more difficult to read,Â but by carefull reading every line of the ouptut you should be able to locate the corresponding operation in the code. \n",
    "The `opcode` column describes the operation : \n",
    "- *placeholder* is an input\n",
    "- *get_attr* means retrieving the `target` parameter for the module, as here with `f0` and `f_mult`\n",
    "- *call_function* means calling the function `target` to given arguments (in the `args` and `kwargs` columns)\n",
    "- *call_method* means calling the method `target` of a given object\n",
    "- *output* represents the output of the computing graph\n",
    "\n",
    "**Activations.** The list of all the intermediary values of the processing graph, that are actually the output of all the operations described by the graph. You can the shape of every activation on the table, that can change for different input shapes in the `.trace(t=t)` step. Analyzing graphs / activations can be a little tedious, but here it can be quite simply done, as with the following examples : \n",
    "\n",
    "- the activation `mul` is the multiplication of `f0` and `f_mult`, and the corresponds to the line `freqs = self.f0 * self.f_mult`. `mul` is then the frequency of each partial.\n",
    "- the activation `sin` is the application of the sinus function on the phase vector, that is here `mul_2`.\n",
    "- the activation `mul_3` is the multiplication of `softmax` (the normalized amplitude for each partial) and `sin`, and then represents the balanced partial.\n",
    "- the activation `sum_1` is the sum of all the partials, hence the final sinewave. \n",
    "\n",
    "Let's plot all the activations corresponding to temporal values (you can distinguish them by checking their shapes : all the `(*,*,88200)` logically corresponds to time series). You can easily do that with `torchbend` using the `get_activations` method : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dandb import plot_1d_activation\n",
    "\n",
    "activations = [\"t\", \"mul_2\", \"sin\", \"mul_3\", \"sum_1\"]\n",
    "activations = bended_module.get_activations(*activations, t=t)\n",
    "\n",
    "for activation_name, activation_value in activations.items():\n",
    "    # plot given activation\n",
    "    plot = plot_1d_activation(activation_name, activation_value)\n",
    "    # show plot\n",
    "    plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important warning** : if you have **any** questions, ask them know! A good understanding of these concepts is very important for what follows this afternoon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And know, what about RAVE?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now we will attack a proper machine learning model : a RAVE model. The `torchbend` use is exactly the same, so let us do write almost the same code than with previous \"dumb\" models : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dandb import download_models, import_model\n",
    "\n",
    "models = download_models()\n",
    "print(\"downloaded models :\", models)\n",
    "\n",
    "# just take the first model\n",
    "current_model = models[\"sol_full_nopqmf\"]\n",
    "bended_model = import_model(current_model)\n",
    "\n",
    "# print paramters\n",
    "print('Weights : ')\n",
    "bended_model.print_weights()\n",
    "\n",
    "print('\\nActivations : ')\n",
    "# print activations\n",
    "bended_model.print_activations();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ouch. Do not worry, we will investigate all of that in the next notebook, it is not as painful as it seems !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
