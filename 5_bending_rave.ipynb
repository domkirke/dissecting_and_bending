{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bending RAVE for weird audio generations\n",
    "\n",
    "Ok, we saw the potentialities of bending with the simple MNIST example ; however, what about RAVE? We had a little flavour in the third notebook of the inner working of RAVE: now, let's use this knowledge to... well, we'll find out! \n",
    "\n",
    "***Note***: we will focus on the decoder, as this is the main sound generator. You can experiment by bending the encoder, but of course bending the encoder would not be able to produce sounds that could not be generated before. Yet, it can be interesting to manipulate the way a incoming sound is encoded to the latent space and transformed, so it's still worth!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchbend as tb; tb.set_output('notebook')\n",
    "from dandb import download_models, import_model\n",
    "models = download_models()\n",
    "current_model = models[\"sol_full_nopqmf\"]\n",
    "model = import_model(current_model)\n",
    "\n",
    "model.print_weights(flt=r\"decoder\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that the convolutional kernels may be selected with the pattern \"decoder.net.{layer}.aligned.branches.0.net.\\d.weight_v\", where {layer} is a specific layer (except the upsampling ones) and \\d is a special character that will match every digit. Upsampling convs are accessible through the \"decoder.net.{layer}.weight_v\". So, let's try to apply our bending operations to these layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dandb import get_sounds, plot_audio\n",
    "\n",
    "x = get_sounds().load(\"violin.wav\")\n",
    "print('original : ')\n",
    "plot_audio(x[0], display=True)\n",
    "z = model.encode(x)\n",
    "out = model.decode(z)\n",
    "print('reconstruction : ')\n",
    "plot_audio(out[0], display=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "layers = [r\"decoder.net.%d.aligned.branches.0.net.\\d.weight_v\"%layer for layer in [3, 8, 12, 18]]\n",
    "# uncomment the line below to bend the upsampling layer instead\n",
    "# layers = [r\"decoder.net.%d.weight_v\"%layer for layer in [2, 6, 11, 16]]\n",
    "\n",
    "# /!\\ CALLBCACKS ZONE_______________________________________________\n",
    "# do not hesitate to try out different callbacks and parameters here! \n",
    "\n",
    "# MASKING\n",
    "# it's marvelous how hard you can discard the weights to have similar sounds. (above 30%, difference is barely noticeable)\n",
    "# these networks should be compressible...\n",
    "# cb = tb.Mask(0.1)\n",
    "\n",
    "# SCALING\n",
    "# as the network is \"almost\" linear, positive scaling often yield overdriven gain\n",
    "# weights are also normalized by the network, so effects are limited\n",
    "# with negative scaling... try that yourself ðŸ¤“\n",
    "cb = tb.Scale(-1.0)\n",
    "\n",
    "# BIASING\n",
    "# contrary to scaling, biasing is VERY sensible... proceeed with caution! \n",
    "# cb = tb.Bias(0.05)\n",
    "\n",
    "# NOISING\n",
    "# same here, noising in additive noise is very sensible (as it comes back to adding stuff)\n",
    "# cb = tb.Normal(std=1., op = \"add\")\n",
    "# cb = tb.Normal(std=1., op = \"mul\")\n",
    "\n",
    "for bended_layer in layers:\n",
    "    model.reset()\n",
    "    model.bend(cb, bended_layer, bend_graph=False, verbose=True)\n",
    "    out = model.decode(z)\n",
    "    print(\"bending %s with %s\"%(bended_layer, cb))\n",
    "    plot_audio(out[0], display=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bending RAVE activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.print_activations(\"decode\", op=\"call_module\", flt=\"decoder.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = ['decoder_net_%d_aligned_branches_1'%i for i in [4, 8, 13, 18]]\n",
    "\n",
    "\n",
    "# /!\\ CALLBCACKS ZONE_______________________________________________\n",
    "# do not hesitate to try out different callbacks and parameters here! \n",
    "\n",
    "# MASKING\n",
    "# it's marvelous how hard you can discard the weights to have similar sounds. (above 30%, difference is barely noticeable)\n",
    "# these networks should be compressible...\n",
    "cb = tb.Mask(0.1)\n",
    "\n",
    "# SCALING\n",
    "# as the network is \"almost\" linear, positive scaling often yield overdriven gain\n",
    "# weights are also normalized by the network, so effects are limited\n",
    "# with negative scaling... try that yourself ðŸ¤“\n",
    "# cb = tb.Scale(-1.0)\n",
    "\n",
    "# BIASING\n",
    "# contrary to scaling, biasing is VERY sensible... proceeed with caution! \n",
    "# cb = tb.Bias(0.05)\n",
    "\n",
    "# NOISING\n",
    "# same here, noising in additive noise is very sensible (as it comes back to adding stuff)\n",
    "# cb = tb.Normal(std=1., op = \"add\")\n",
    "cb = tb.Normal(std=1., op = \"mul\")\n",
    "\n",
    "for bended_act in layers:\n",
    "    model.reset()\n",
    "    model.bend(cb, bended_act, bend_param=False, verbose=True)\n",
    "    out = model.decode(z)\n",
    "    print(\"bending %s with %s\"%(bended_layer, cb))\n",
    "    plot_audio(out[0], display=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
