{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys; sys.path.append('../torchbend')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bending a MNIST auto-encoder\n",
    "\n",
    "This notebook has be conceived to introduce you to the notion of *network bending*, and let you perform simple bending operations on a little image generative model, that is generally more intuitive than large audio models.\n",
    "\n",
    "## What is bending? \n",
    "\n",
    "The idea of *network bending*, first referenced by [Terence Broads](https://arxiv.org/pdf/2005.12420), is the equivalent of [circuit bending](https://fr.wikipedia.org/wiki/Circuit_bending) for neural network-based models : hijacking a model developed and designed for a purpose (here, the original task of the machine learning model) by the alteration of its inner circuiteries. You can also see that as opening a *modular* approach to machine learning, while this idea of *hijack* is still important and not naturally encompassed by modular synthesis. `torchbend` is a library designed originally to allow high-end functions for graph & parameter bending, allowing to generate data in a way that would not have been possible without these kind of alterations. This is why it can also be used for *dissection*, as being able to develop an understanding of how the model work is very important to not lose one's time perfoming alterations that would not have any sense, exactly like bending analogical devices. \n",
    "\n",
    "Ready to go? Let's bend this little image model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from dandb.networks import make_mnist_vae\n",
    "import torchbend as tb\n",
    "\n",
    "vae = make_mnist_vae()\n",
    "state_dict = torch.load('models/original/mnist_vae/final.ckpt', map_location=torch.device('cpu'))\n",
    "vae.load_state_dict(state_dict)\n",
    "bended_vae = tb.BendedModule(vae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight bending\n",
    "\n",
    "A way of bending a module is to bend its *parameters*, or *weights*. The cool part about weight bending is also that it is available for every model written with PyTorch, while other bending techniques are unfortunately dependant of the coding style of the target module (that's silly, but true). Let's try that with our module : the only thing you have to do is to call the `bend` method with a weight key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dandb import plot_image_activations\n",
    "\n",
    "names = bended_vae.resolve_parameters(r\"decoder.net.convt.*weight.*\")\n",
    "print(names)\n",
    "\n",
    "# we will here sample the latent space, as we will only bend the decoder.\n",
    "n_samples = 16\n",
    "z = torch.randn(n_samples, 64)\n",
    "# we trace the decode function\n",
    "bended_vae.trace(\"decode\", z=z)\n",
    "\n",
    "# we will use the Mask bending callback, that destroys the weight / activation\n",
    "# with a given probability rate. \n",
    "\n",
    "# we keep 20% of the mask\n",
    "mask = 0.2\n",
    "# we put this into a BendingParameter object ; we'll see why later\n",
    "param = tb.BendingParameter('mask', mask)\n",
    "# init the callback\n",
    "cb = tb.Mask(prob=param)\n",
    "\n",
    "outs = []\n",
    "for weight_name in names:\n",
    "    # reset previous bendings\n",
    "    bended_vae.reset()\n",
    "    bended_vae.bend(cb, weight_name)\n",
    "    out = bended_vae.decode(z=z)\n",
    "    outs.append(out)\n",
    "\n",
    "plot_image_activations(outs, n_rows=1, display=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that weights have different effects on the produced output, depending on their proximity to the output. Let's apply a scale transformation : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dandb import plot_image_activations\n",
    "\n",
    "names = bended_vae.resolve_parameters(r\"decoder.net.convt.*weight.*\")\n",
    "print(names)\n",
    "\n",
    "# we will here sample the latent space, as we will only bend the decoder.\n",
    "n_samples = 16\n",
    "z = torch.randn(n_samples, 64)\n",
    "# we trace the decode function\n",
    "bended_vae.trace(\"decode\", z=z)\n",
    "\n",
    "# we will use the Mask bending callback, that destroys the weight / activation\n",
    "# with a given probability rate. \n",
    "\n",
    "\n",
    "# scale all the weights by 10\n",
    "scale = 10\n",
    "# or, invert all the weights (uncomment)\n",
    "# scale = -1\n",
    "\n",
    "\n",
    "# we put this into a BendingParameter object ; we'll see why later\n",
    "param = tb.BendingParameter('scale', scale)\n",
    "# init the callback\n",
    "cb = tb.Scale(scale=param)\n",
    "\n",
    "outs = []\n",
    "for weight_name in names:\n",
    "    # reset previous bendings\n",
    "    bended_vae.reset()\n",
    "    bended_vae.bend(cb, weight_name)\n",
    "    out = bended_vae.decode(z=z)\n",
    "    outs.append(out)\n",
    "\n",
    "plot_image_activations(outs, n_rows=1, display=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the network is *almost* linear, which is quite normal as convolutional operators are linear and the nonlinearity used here acts like a saturation unit. If you invert the weights though, the output is much different ; this is because the nonlinearity `ReLU` is asymmetrical. Let's try with a bias now :  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dandb import plot_image_activations\n",
    "\n",
    "names = bended_vae.resolve_parameters(r\"decoder.net.convt.*weight.*\")\n",
    "print(names)\n",
    "\n",
    "# we will here sample the latent space, as we will only bend the decoder.\n",
    "n_samples = 16\n",
    "z = torch.randn(n_samples, 64)\n",
    "# we trace the decode function\n",
    "bended_vae.trace(\"decode\", z=z)\n",
    "\n",
    "# we will use the Mask bending callback, that destroys the weight / activation\n",
    "# with a given probability rate. \n",
    "\n",
    "\n",
    "# bias positively all the weights\n",
    "bias = 0.1\n",
    "# or negatively\n",
    "# bias = -0.1\n",
    "\n",
    "\n",
    "# we put this into a BendingParameter object ; we'll see why later\n",
    "param = tb.BendingParameter('bias', bias)\n",
    "# init the callback\n",
    "cb = tb.Bias(bias=param)\n",
    "\n",
    "outs = []\n",
    "for weight_name in names:\n",
    "    # reset previous bendings\n",
    "    bended_vae.reset()\n",
    "    bended_vae.bend(cb, weight_name)\n",
    "    out = bended_vae.decode(z=z)\n",
    "    outs.append(out)\n",
    "\n",
    "plot_image_activations(outs, n_rows=1, display=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The impact of the bias is more dramatic : indeed, this biases all the activations, but the effect is also very different across layers. You can experiment any function you want with the `Lambda` callback : \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dandb import plot_image_activations\n",
    "\n",
    "names = bended_vae.resolve_parameters(r\"decoder.net.convt.*weight.*\")\n",
    "print(names)\n",
    "\n",
    "# we will here sample the latent space, as we will only bend the decoder.\n",
    "n_samples = 16\n",
    "z = torch.randn(n_samples, 64)\n",
    "# we trace the decode function\n",
    "bended_vae.trace(\"decode\", z=z)\n",
    "\n",
    "# bias positively all the weights\n",
    "bias = 0.1\n",
    "# or negatively\n",
    "# bias = -0.1\n",
    "\n",
    "# we put this into a BendingParameter object ; we'll see why later\n",
    "param = tb.BendingParameter('bias', bias)\n",
    "# init the callback\n",
    "\n",
    "def bending_op(x, f=8):\n",
    "    return torch.cos(2 * f * torch.pi * x)\n",
    "\n",
    "cb = tb.Lambda(bending_op)\n",
    "\n",
    "outs = []\n",
    "for weight_name in names:\n",
    "    # reset previous bendings\n",
    "    bended_vae.reset()\n",
    "    bended_vae.bend(cb, weight_name)\n",
    "    out = bended_vae.decode(z=z)\n",
    "    outs.append(out)\n",
    "\n",
    "plot_image_activations(outs, n_rows=1, display=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how, by applying effects on the model's weights, we can have important effect on the produced output in a way that could not be achievable without altering the network. Yet, this method bends all the inputs in the same manner. Activation bending allows more subtle way of making different operations for different inputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation bending\n",
    "\n",
    "The other way to bend a module is by directly modulating its intermediary processing values, the *activations*. The syntax is strictly similar, except that the keys must be one of the activations listed in the activation list. If you to be sure to only bend activations, you can add the `bend_activation` keyword.\n",
    "\n",
    "We will use here the `tb.ThresholdActivation` bending callback, that only keeps a given proportion of the active channels of an activation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dandb import plot_image_activations\n",
    "\n",
    "bended_vae.reset()\n",
    "names = list(sorted(bended_vae.resolve_activations(r\"decoder_net_convt.*\", fn=\"decode\")))\n",
    "print(names)\n",
    "\n",
    "# we will here sample the latent space, as we will only bend the decoder.\n",
    "n_samples = 16\n",
    "z = torch.randn(n_samples, 64)\n",
    "# we trace the decode function\n",
    "bended_vae.trace(\"decode\", z=z)\n",
    "\n",
    "# here, we filter half of the less activated features\n",
    "# change the float number below to adjsut the amount of channels kept\n",
    "param = tb.BendingParameter('threshold', 0.5)\n",
    "# init the callback\n",
    "cb = tb.ThresholdActivation(threshold=param, dim=-3, invert=False)\n",
    "\n",
    "outs = []\n",
    "for activation_name in names:\n",
    "    # reset previous bendings\n",
    "    bended_vae.reset()\n",
    "    bended_vae.bend(cb, activation_name)\n",
    "    out = bended_vae.decode(z=z)\n",
    "    outs.append(out)\n",
    "\n",
    "plot_image_activations(outs, n_rows=1, display=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With activation bending, the effect can be different for every output, while weight bending has the same effect on every input. For example, instead of masking the channels of lower amplitude, we can normalize all the channels by instance : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dandb import plot_image_activations\n",
    "\n",
    "bended_vae.reset()\n",
    "names = list(sorted(bended_vae.resolve_activations(r\"decoder_net_convt.*\", fn=\"decode\")))\n",
    "\n",
    "# we will here sample the latent space, as we will only bend the decoder.\n",
    "n_samples = 16\n",
    "z = torch.randn(n_samples, 64)\n",
    "\n",
    "def norm_by_instance(activation):\n",
    "    # activtion shape is (batch x channel x height x width), so\n",
    "    # we normalize here across instances\n",
    "    return activation / activation.amax(0, keepdim=True)\n",
    "\n",
    "param = tb.BendingParameter('threshold', 0.5)\n",
    "cb = tb.Lambda(norm_by_instance)\n",
    "\n",
    "outs = []\n",
    "for activation_name in names:\n",
    "    # reset previous bendings\n",
    "    bended_vae.reset()\n",
    "    bended_vae.bend(cb, activation_name)\n",
    "    out = bended_vae.decode(z=z)\n",
    "    outs.append(out)\n",
    "\n",
    "print(\"normalization across instances : \")\n",
    "plot_image_activations(outs, n_rows=1, display=True)\n",
    "\n",
    "\n",
    "def norm_by_channel(activation):\n",
    "    # activtion shape is (batch x channel x height x width), so\n",
    "    # we normalize here across instances\n",
    "    return activation / activation.amax(1, keepdim=True)\n",
    "\n",
    "param = tb.BendingParameter('threshold', 0.5)\n",
    "cb = tb.Lambda(norm_by_channel)\n",
    "\n",
    "outs = []\n",
    "for activation_name in names:\n",
    "    # reset previous bendings\n",
    "    bended_vae.reset()\n",
    "    bended_vae.bend(cb, activation_name)\n",
    "    out = bended_vae.decode(z=z)\n",
    "    outs.append(out)\n",
    "\n",
    "print('normalization across channels : ')\n",
    "plot_image_activations(outs, n_rows=1, display=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we mixed activations? We can try that out with the `tb.InterpolateActivation` callback, that takes as additional input a mixing matrix that allows to make linear interpolation across a set of input activations. With this callback, we will levearge the `from_activations` method, the exact complementary of `get_activations`, that allows you to directly feed activations to a sub part of the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from dandb import plot_image_activations\n",
    "\n",
    "bended_vae.reset()\n",
    "\n",
    "names = list(sorted(bended_vae.resolve_activations(r\"decoder_net_convt.*\", fn=\"decode\")))\n",
    "\n",
    "n_samples = 2\n",
    "val_set = datasets.MNIST(root='datasets/MNIST', train=False, download=True, transform = transforms.ToTensor())\n",
    "examples = torch.stack([val_set[0][0], val_set[100][0]])\n",
    "z = bended_vae.encode(examples)[0]\n",
    "bended_vae.trace(\"decode\", z=z)\n",
    "\n",
    "n_interp = 8\n",
    "cb = tb.InterpolateActivation()\n",
    "\n",
    "outs = []\n",
    "for activation_name in names:\n",
    "    # reset previous bendings\n",
    "    bended_vae.reset()\n",
    "    bended_vae.bend(cb, activation_name)\n",
    "    out_activation = bended_vae.get_activations(activation_name, z=z, fn=\"decode\", _filter_bended=True)\n",
    "    # make linear interpolation between these two examples\n",
    "    interp_weights = torch.stack([torch.linspace(0., 1., n_interp), torch.linspace(1, 0, n_interp)], 1)\n",
    "    out = bended_vae.from_activations(activation_name, **out_activation, interp_weights=interp_weights, fn=\"decode\")\n",
    "    outs.append(out)\n",
    "\n",
    "print(\"layer-wise interpolation across instances : \")\n",
    "plot_image_activations(outs, n_rows=1, display=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the more we perform interpolation on higher layers of the decode (closer to the latent space), \"smoother\" is the interpolation. This reflect the idea quite general in deep learning that higher level generally represent more abstract features, while lower layers (closer to the data) represent more \"local\" features (in this example, we can see that it is closer to a direct \"linear\" interpolation of images). Is it the case with RAVE? Let's try that out! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
