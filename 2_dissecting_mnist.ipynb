{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is MNIST? \n",
    "\n",
    "MNIST a standard dataset back from the 80s that was used to experiment with convolutional neural networks, and still used today as a toy dataset for research. It is simply a dataset of about 80k handwritten labeled digits, very convenient to experiment some stuff as there are 10 defined & balanced classes, each having enough variability to evalute the generalization abilities of our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "from dandb import plot_image_grid\n",
    "\n",
    "val_set = MNIST(root='datasets/MNIST', train=False, download=True, transform = transforms.ToTensor())\n",
    "plot_image_grid(torch.stack([val_set[i][0] for i in range(32)]), display=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's bend\n",
    "### First step :  how does a Variational Auto-Encoder (VAE) work\n",
    "\n",
    "Before entering the VAE's bending, we have to import the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dandb.networks import make_mnist_vae\n",
    "vae = make_mnist_vae()\n",
    "state_dict = torch.load('models/original/mnist_vae/final.ckpt', map_location=torch.device('cpu'))\n",
    "vae.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's reconstruct a . batch of example using the little auto-encoder.  [Variational auto-encoders](https://www.ee.bgu.ac.il/~rrtammy/DNN/StudentPresentations/2018/AUTOEN~2.PDF) are based on two modules : \n",
    "- an *encoder*, that outputs a latent normal distribution in the latent space : $q(\\mathbf{z|x}) = \\mathcal{N}(\\boldsymbol{\\mu}_\\theta(\\mathbf{x}), \\boldsymbol{\\sigma}^2_\\theta(\\mathbf{x}))$\n",
    "- a *decoder*, that outputs a distribution (here Bernoulli has the data is binary) in the latent space : $p(\\mathbf{x|z}) = \\mathcal{N}(\\boldsymbol{\\mu}_\\phi(\\mathbf{z}), \\boldsymbol{\\sigma}^2_\\phi(\\mathbf{z}))$\n",
    "\n",
    "while decoders were initially probabilistic, nowadays everybody gave up this idea; a determinstic output can actually be seen as a normal with zero variance in the limit (infinitely narrowed gaussian). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dandb import plot_image_reconstructions\n",
    "# Load a batch of digits from MNIST\n",
    "\n",
    "batch_size = 64\n",
    "digit_loader = torch.utils.data.DataLoader(val_set, batch_size, shuffle=False, num_workers=0)\n",
    "x = next(iter(digit_loader))[0]\n",
    "\n",
    "# Encode it into its latent representation\n",
    "# Remember, it is equivalent to knobs of a synth (here, a digit synth)\n",
    "mu, var = vae.encode(x)\n",
    "# sample the latent distribution\n",
    "latent_representation, _ = vae.reparametrize(mu, var)\n",
    "\n",
    "# Use this latent representation to generate the output\n",
    "y = vae.decode(latent_representation)\n",
    "\n",
    "plot_image_reconstructions(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the reconstruction is slightly blurry compared to the input. This is because of the probabilistic latent space, that enforces the model to smooth its generations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second step : bending and tracing our digit synth using torchbend\n",
    "\n",
    "As we did with RAVE, let us summarize the activations of our little VAE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchbend as tb\n",
    "tb.set_output('notebook')\n",
    "\n",
    "bended_vae = tb.BendedModule(vae)\n",
    "bended_vae.trace(x=x)\n",
    "\n",
    "activation_names = bended_vae.activation_names()\n",
    "activation_shapes = list(map(bended_vae.activation_shape, activation_names))\n",
    "print('forward method : ')\n",
    "bended_vae.print_graph();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph of the encoder (resp. decoder) is much easier, and consists actually in a series of convolutional (resp. transposed convolutional) operations, followed by batch normazliation and non-linearity. The interesting activations are then much easier to retrieve, as we do here by plotting all the activations for encoding a single example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dandb import plot_image_activations\n",
    "\n",
    "# change the below number to change the displayed example!\n",
    "batch_idx = 4\n",
    "act_names = [f'encoder_net_act{i}' for i in range(4)]#+[f'decoder_net_act{i}' for i in range(2, 5)]\n",
    "outs = bended_vae.get_activations(*act_names, x=x, fn='forward')\n",
    "\n",
    "for act in act_names:\n",
    "    plot_act = outs[act][batch_idx]\n",
    "    plot_image_activations(plot_act, display=True, name=act, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inversely, with the decoder: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dandb import plot_image_activations\n",
    "\n",
    "# change the below number to change the displayed example!\n",
    "batch_idx = 4\n",
    "act_names = [f'decoder_net_act{i}' for i in range(2, 5)]\n",
    "outs = bended_vae.get_activations(*act_names, x=x, fn='forward')\n",
    "\n",
    "for act in act_names:\n",
    "    plot_act = outs[act][batch_idx]\n",
    "    plot_image_activations(plot_act, display=True, name=act, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By doing that, we can see all the intermediary values that are processed to encode and decode a given example.\n",
    "\n",
    "### Dissecting the weights\n",
    "\n",
    "Let's summarize the weights of our model : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bended_vae.print_weights(r\".*conv\\d.weight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that, for each layer, the kernels are $(n_{out}\\times n_{in})$ little patches of $(5\\times5)$ dimensions., that we can plot individually : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_kernels = 128\n",
    "for i in range(4):\n",
    "    kernels = bended_vae.state_dict()[f'encoder.net.conv{i}.weight']\n",
    "    kernels = kernels.reshape(-1,5,5)[:max_kernels] \n",
    "    # normalize by kernel for visualization\n",
    "    kernels = kernels / kernels.amax(0)[None]\n",
    "    plot_image_activations(kernels, display=True, name=\"kernels for layer %d\"%i, height=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that none of these weights has a clear sense ; but somehow, the model manage to perform the task we asked it to do. This is typical of deep learning, and prevents to have a clear understanding on the responsability of a given part of a network ; and, that is normal, as every unit influences the whole network, such that this very tight entanglement is part of the network's efficiency.  In the next notebook, we will dissect an even more complex network : RAVE. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
