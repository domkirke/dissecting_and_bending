{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dissecting RAVE\n",
    "\n",
    "Ok, now let's take our scalpel and dissemble RAVE's encoder and decoder. Remembering our detailed schema : \n",
    "\n",
    "![RAVE detailed](assets/rave_detailed.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "we remember that the encoder is composed of \n",
    "- a *filter bank* (pseudo-quadratic mirror filtering)\n",
    "- a sequence of N strided convolution blocks (one conv, one norm, one activation)\n",
    "\n",
    "\n",
    "and that the decoder is composed of \n",
    "- a sequence of residual convolutional blocks\n",
    "- followed by an optional filtered noise, a loudness coefficient, and a waveform obtained from the residual blocks.\n",
    "\n",
    "actually, we will use all along this session a *no-PQMF* version of RAVE for the decoder, for reasons we will explain later. We will use `torchbend` to analyse these different layers, and get a more precise idea of the inner DSP of RAVE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dissecting the encoder\n",
    "Let's dissect the encoder of RAVE : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.append('../torchbend')\n",
    "import torch\n",
    "import torchbend as tb; print(tb.__file__)\n",
    "tb.set_output('notebook')\n",
    "from dandb import download_models, import_model\n",
    "\n",
    "model_list = download_models()\n",
    "model = import_model(model_list[\"sol_full_nopqmf\"])\n",
    "print(model.encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print out all the steps of the computing graph that calls an inner module, to have an idea of which submodule of the encoder is called along the encoding process: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "# trace the forward function\n",
    "# model.trace(x=torch.randn(1,1,2048), fn=\"encode\")\n",
    "model.print_graph(\"encode\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, this is a very complex graph ; indeed, every suboperation (even accessing a subitem of a tensor for example) is graphed, and we can access ALL of them. This a little tedious, especially with an architecture like RAVE's that is based on complex convolutional modules (actually residual and multi-dilation). This makes the analysis tedious, but with a bit of a detective's attitude we will manage to find what we need. First, let's filter all the \"uninteresting\" steps such as attribut getting, getitem, reshape, copy, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.print_graph(\"encode\", exclude=[r'getattr', r'getitem', r'copy', r'cat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, this is a bit more readable, but still hard to decipher. Though, we can notice that : \n",
    "1) the encoder is regularly accessing convolutional weights (`encoder.encoder.net.3.aligned.branches.0.net.1.weight_v` for example) and applying convoluational operations\n",
    "2) encoder is also regularly applying `sin` and `pow`, ressembling to an activation function\n",
    "\n",
    "For the 1st point, let's check that : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.print_activations(\"encode\", flt='conv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observing the shape to the left of the table (and remembering that the shapes are `n_batch, n_channels, n_steps`), we can see that every 8 layers the outputs are downsampled in time, and the channels increased. For our point 2), let's check quickly the configuration of the `.gin` file of the model : \n",
    "\n",
    "```\n",
    "...\n",
    "\n",
    "# Macros:\n",
    "# ==============================================================================\n",
    "ACTIVATION = @blocks.Snake\n",
    "```\n",
    "\n",
    "ok, the activation blocks used here are `Snake`, whose definition can be found in `RAVE/rave/blocks.py` after a quick search : \n",
    "```\n",
    "def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "    return x + (self.alpha + 1e-9).reciprocal() * (self.alpha *\n",
    "                                                    x).sin().pow(2)\n",
    "```\n",
    "so it seems that these `pow` operations could indeed correspond the activation output of each layer. Let's check that : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.print_activations(\"encode\", flt='pow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, in the Snake activation the activations are a little operations later, after a multiplication and an addition with another added value. Then, let's locate the `add_*` `call_functions` that are after these `pow_*` activations :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.print_activations(\"encode\", flt='add')\n",
    "model.print_graph(\"encode\", flt='add');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "*Bingo!* By double-checking shapes and arguments of respectively activations and graph (args of type `add_*, mul_*`), it seems that the activations we want are `add_4`, `add_9` , `add_14`, and `add_18`. Though, we're not over yet ; indeed, we miss the output of the final layer, that is of size `n_examples x 256 x n_seq`, where 256 is the double of the model's latent size because both mean and standard deviation are encoded (like in traditional variational auto-encoders). We give it to you here, this activation is called `conv1d_28` and is the final convolution that linearly transforms 1024 channels into 256. \n",
    "\n",
    "Now that we have the activations we want to target, let's retrieve them using the `get_activations` method of `torchbend` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from dandb import get_sounds, plot_1d_activation\n",
    "\n",
    "sounds = get_sounds()\n",
    "x = sounds.load('violin.wav', sr=model.sample_rate)\n",
    "\n",
    "activations = [*['add_%d'%i for i in [4,9,14,18]], 'conv1d_28']\n",
    "out = model.get_activations(*activations, fn=\"encode\", x=x)\n",
    "out['conv1d_28'] = out['conv1d_28'][:, :128] # we delete the std part of the latent encoding\n",
    "\n",
    "\n",
    "# change the number below to update the number of plotted dimensions! \n",
    "n_plot_dims = 16\n",
    "for k, v in out.items():\n",
    "    print(v.shape)\n",
    "    act_max = torch.argsort(v.amax(dim=[0, 2]), descending=True)[:16]\n",
    "    plot_1d_activation(k, v[:, act_max], display=True, channel_idx=act_max.tolist()) \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so this is what internal encoder's activations look like. We can see that the period aspects of the activations is quite high, explaining why the latent representations can seem so messy when using a scope of the representation of the RAVE VST. We can also observe some kind of \"flattening\" or \"normalization\" effect : at the very beginning of the encoding process the amplitudes roughly follow the amplitude of the sound, while the latter activations get saturated, and more abstract, besides being drastically downsampled (but the number of channels are increasing, such that globally most of the dimensionality of the input is compressed at the very end of the encoding process).  \n",
    "\n",
    "Features of the encoder can then be understood as the encoding of local features (indexed by the channel index) representing an increasing temporal scope, finally piped to the latent space that actually performs most of the compression. So, what about the decoder?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dissecting the decoder\n",
    "\n",
    "Actually, the decoder is roughly speaking the reverse process : a sequence of upsampling convolutions. If you want, try finding yourself the activations to plot as we did for the encoder! If the courage (or interest) is missing, you can jump the next cell and uncomment the last line of the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dandb\n",
    "\n",
    "# try finding the good activations to plot by modifying the flt or exclude keywords below! \n",
    "flt = \"\"\n",
    "exclude = None\n",
    "model.print_graph(\"decode\", flt=flt, exclude=exclude)\n",
    "model.print_activations(\"decode\", flt=flt, exclude=exclude)\n",
    "\n",
    "target_activations = []\n",
    "# discouraged or bored? uncomment the line below : \n",
    "target_activations = dandb.RAVE_DECODER_ACT_NAMES_DECODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, let's also plot them as we did for the encoder's activations :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dandb\n",
    "from IPython.display import Audio, display\n",
    "from torchaudio.functional import resample\n",
    "\n",
    "sounds = get_sounds()\n",
    "x = sounds.load('violin.wav', sr=model.sample_rate)\n",
    "n_samples = x.shape[-1]\n",
    "z = model.encode(x=x)\n",
    "\n",
    "\n",
    "out = model.get_activations(*target_activations, fn=\"decode\", z=z)\n",
    "\n",
    "# change the number below to update the number of plotted dimensions! \n",
    "n_plot_dims = 16\n",
    "for k, v in out.items():\n",
    "    print(k)\n",
    "    act_max = torch.argsort(v.amax(dim=[0, 2]), descending=True)[:16]\n",
    "    plot_1d_activation(k, v[:, act_max], display=True, channel_idx=act_max.tolist()) \n",
    "\n",
    "    # export as audio files the intermediary activations\n",
    "    sample_rate = (v.shape[-1] / n_samples) * model.sample_rate\n",
    "    v_sum = v.sum(-2)\n",
    "    v_sum = v_sum / v_sum.amax()\n",
    "    v_sum = resample(v_sum, int(sample_rate), 44100)\n",
    "    audio = Audio(v_sum.numpy(), rate=44100)\n",
    "    display(audio)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The upsampling process that is inherent to the RAVE's decoding process is clear here : each step, consisting in an upscaling layer and a sequence of convolutional operations, can be though as a transformation from the latent starting from the low-frequency compontent to the higher frequencies components. The process is non-linear though, such as this process is not exactly assimilable to a \"bass2treble\" generation ; though, the idea is here. This process is actually the same in decoding-based image generation models such as VAEs or StyleGANs, where the image is progressively upsampled and details added in the generation process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What about the weights? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1-d convolutional operations is mathematically defined as follows : \n",
    "\n",
    "$\\text{out}(N_i, C_{\\text{out}_j}) = \\text{bias}(C_{\\text{out}_j}) + \\sum_{k = 0}^{C_{in} - 1} \\text{weight}(C_{\\text{out}_j}, k) \\star \\text{input}(N_i, k)$\n",
    "\n",
    "where ⋆ is the valid cross-correlation operator, $N$ is a batch size, $C$ denotes a number of channels, $L$ is a length of signal sequence. RAVE uses un-biased convolutions, such as convolutions are only represented by their weights of shape `[channels_out, channels_in, kernel_size]`. RAVE also uses strided transposed convolutions, upsampling at the same time the signal and allowing multi-resolution generation (no stride to the left, stride to the right): \n",
    "\n",
    "![](assets/conv_transpose_nostride.gif) ![](assets/conv_transpose_stride.gif)\n",
    "\n",
    "We can location the weights of RAVE's encoder with the following request : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.print_weights(flt=r\"encoder.*weight_v\")\n",
    "model.print_weights(flt=r\"encoder.*weight_v\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, how to represent all the kernels of a convolutional operation? Well, by plotting all the 1-to-1 kernel from input to output dimensions ; however, this can be way to large for certain cases, as ours. For exemple, even for the lightest layer there are still 64 * 64 = 4096 convolutions to plot, that is still huge. This is why deep learning models are so difficult to apprehend and are said to be *black-box* : having a concrete idea of the idea of each seperate part is most of the time impossible, as the dimensionality is too high and the inner behavior highly non-linear.\n",
    "\n",
    "Is this a reason to give up? No! By looking closely, we can see that there are sveral type of convolutions : \n",
    "- the convolutions with 1-d kernels, that can be imagined as \"mixing\" operations between channels\n",
    "- the convolutions with 3-d kernels, that are \"proper\" convolution operations\n",
    "- the convolutions with 8-d kernels and channel augmentation (convolutions with indexes `[0,5,10,15,19]`)\n",
    "\n",
    "Let's focus on second last ones, plot the amplitude of each convolution kernel, trying to see if some can be avoided (again with this amplitude criterion, that is discutable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dandb import plot_kernel_grid\n",
    "from plotly.express import histogram\n",
    "# possible layers : \n",
    "weight_name = \"encoder.encoder.net.2.aligned.branches.0.net.1.weight_v\"\n",
    "param = model.state_dict()[weight_name]\n",
    "\n",
    "# mix first and second dimension\n",
    "print(param.shape)\n",
    "param = param.reshape(-1, param.shape[-1])\n",
    "print(param.shape)\n",
    "# take L2 norm\n",
    "param_norm = param.pow(2).sum(-1).sqrt()  \n",
    "\n",
    "histogram({'amplitude': param_norm.numpy()}, x=\"amplitude\", height=200).show()\n",
    "\n",
    "sorted_idx = torch.argsort(param_norm, descending=True)\n",
    "n_kernels = 64\n",
    "plot_kernel_grid(param[sorted_idx[:n_kernels]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, these kernels are not very informative.... Let's plot the longer ones : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dandb import plot_kernel_grid\n",
    "from plotly.express import histogram\n",
    "# possible layers : \n",
    "for layer in [0,5,10,15,19]:\n",
    "    weight_name = f\"encoder.encoder.net.{layer}.weight_v\"\n",
    "    print(weight_name)\n",
    "    param = model.state_dict()[weight_name]\n",
    "    # mix first and second dimension\n",
    "    param = param.reshape(-1, param.shape[-1])\n",
    "    # take L2 norm\n",
    "    param_norm = param.pow(2).sum(-1).sqrt()  \n",
    "\n",
    "    histogram({'amplitude': param_norm.numpy()}, x=\"amplitude\", height=200).show()\n",
    "\n",
    "    sorted_idx = torch.argsort(param_norm, descending=True)\n",
    "    n_kernels = 64\n",
    "    plot_kernel_grid(param[sorted_idx[:n_kernels]], display=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cannot say these weights are much more informative ; yet, this quickly give an idea of the \"filterbank\" of the successive downsampling layers of the encoder, and we can see that filters closer to the waveform have learned some kind of periodicity and are quite simalar, while the kernels in the higher level differentiate a little more. All these kernels can be thought as the \"dictionary\" of the encoding, the plotted ones being the most significant words. \n",
    "\n",
    "### What about the decoder?\n",
    "\n",
    "Let's do the same kind of operation with the decoder : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.print_weights(flt=r\"decoder.*weight_v\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the process is very similar. Let's plot our most significant convolution weights for layers with channel reduction :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dandb import plot_kernel_grid\n",
    "from plotly.express import histogram\n",
    "# possible layers : \n",
    "for layer in [2,6,11,16,21]:\n",
    "    weight_name = f\"decoder.net.{layer}.weight_v\"\n",
    "    print(weight_name)\n",
    "    param = model.state_dict()[weight_name]\n",
    "    # mix first and second dimension\n",
    "    param = param.reshape(-1, param.shape[-1])\n",
    "    # take L2 norm\n",
    "    param_norm = param.pow(2).sum(-1).sqrt()  \n",
    "\n",
    "    histogram({'amplitude': param_norm.numpy()}, x=\"amplitude\", height=200).show()\n",
    "\n",
    "    sorted_idx = torch.argsort(param_norm, descending=True)\n",
    "    n_kernels = 64\n",
    "    plot_kernel_grid(param[sorted_idx[:n_kernels]], display=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How is this informative? Not very much. However, this allows to get a feeling of what's happening inside, looking at all this little wavelets that are summed and processed along the decoding process. Especially, visualizing this can give some ideas to actually *mess up* with these kernels and activations, knowing a little bit more what to target and which modifications to do. We will see that this afternoon! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
